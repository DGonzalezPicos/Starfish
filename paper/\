%\documentclass[manuscript]{aastex} %one-column, double-spaced document
\documentclass[preprint]{aastex} %double-column, single-spaced document:
%\documentclass[iop,floatfix]{emulateapj} 

\usepackage{hyperref}
%\usepackage{graphicx}
%\usepackage{apjfonts}
\usepackage{enumerate}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{bm}

\newcommand{\prob}{{\rm prob}}
\newcommand{\qN}{\{q_i\}_{i=1}^N}
\newcommand{\yN}{\{y_i\}_{i=1}^N}
\newcommand{\vt}{\vec{\theta}}
\newcommand{\vstar}{\vt_{\star}}
\newcommand{\vN}{\vt_{\rm N}}
\newcommand{\vc}{\vec{c}}
\newcommand{\fM}{f_{\rm M}}
\newcommand{\fD}{f_{\rm D}}
\newcommand{\vD}{\vec{D}}
\newcommand{\dd}{\,{\rm d}}
\newcommand{\trans}{\mathsf{T}}

%\slugcomment{}
%\shorttitle{}
%\shortauthors{}

\begin{document}

\title{A Bayesian method for inference of fundamental stellar parameters}
\author{\today{}\\
\medskip
Ian~Czekala\altaffilmark{1} and Sean M. Andrews\altaffilmark{1}
%Author2\altaffilmark{2},
}

\altaffiltext{1}{Harvard-Smithsonian Center for Astrophysics, 60 Garden Street MS 10, Cambridge, MA 02138}
%\altaffiltext{2}{Institution 2}
\email{iczekala@cfa.harvard.edu}

%\begin{abstract}
%\end{abstract}
%\keywords{}

%\begin{figure}[htb]
%\begin{center}
%\includegraphics{file}
%\caption{}
%\label{fig:}
%\end{center}
%\end{figure}

\section{The model}

To summarize our model, we use Bayes rule to design a posterior probability function
\begin{equation}
  \overbrace{p(\vt | \vD)}^{\rm posterior} \propto \underbrace{p(\vD | \vt)}_{\rm likelihood} \; \overbrace{p(\vt)}^{\rm prior}
  \label{eqn:posterior}
\end{equation}
where the parameter vector is comprised of both stellar parameters and calibration parameters $\vt = \{\vstar, \vN \}$. Stellar parameters include those mentioned previously, $\vstar = \{M_\star, T_{\rm eff}, \log(g), v \sin i, v_z, A_v \}$. The data vector $\vD = \fD(\lambda)$ represents the data spectrum we wish to use to infer stellar parameters. In the case of our companion paper, this is a 51-order echelle spectrum from TRES at a resolution of $R=48,000$ ($6.8\;{\rm km/s}$) and spanning the full optical range.

Although per-pixel photon counting errors are Poisson, we can safely assume that we have enough counts that we are in the Gaussian limit. Then our likelihood function becomes a pixel-by-pixel $\chi^2$ comparison between the data spectrum $\fD$, and the model spectrum $\fM$, summed over the wavelength axis.\footnote{We must necessarily mask certain wavelength regions of the T Tauri spectra that are contaminated (one might also say ``made more interesting'') by the astrophysical realities of accretion or stellar spots (such as the Balmer lines, which are in emission).  We also accept the assumption that rotation or accretion onto the star does not fundamentally alter the structure of the model atmosphere that generates the model spectrum and that a spectral comparison of the ``clean'' regions of the spectrum is valid.} 
\begin{equation}
 p(\vD | \vt) = {\cal L} \propto \exp \left(-\frac{\chi^2}{2} \right)
\end{equation}

\begin{equation}
  \chi^2 = \sum_\lambda \left [\frac{\fM(\lambda | \vstar) - k(\lambda | \vN) \fD(\lambda) }{\sigma(\lambda)} \right ]^2
\end{equation}
Assuming flat priors for the moment (an assumption we will later relax), then the logarithm of our posterior probability function is
\begin{equation}
  \ln \bigl (p(\vt | \vD) \bigr) \propto - \frac{1}{2} \sum_\lambda \left [\frac{\fM(\lambda | \vstar) - k(\lambda | \vN) \fD(\lambda) }{\sigma(\lambda)} \right ]^2
  \label{eqn:lnprob}
\end{equation}
To generate a model spectrum given a set of parameters $\fM(\vt)$, a model spectrum is created by linearly interpolating between grid points of a high-resolution PHOENIX model stellar spectra ($R = 500,000$), rotationally broadened, instrumentally broadened, Doppler shifted, extinction corrected, and downsampled to the exact pixels of the TRES spectrum. $\sigma(\lambda)$ is the RMS scatter of the continuum (measured in a line-free region) inversely scaled by the blaze function in each order, to account for Poisson photon counting errors. For some data reduction pipelines, a per-pixel ``sigma spectrum'' that accounts for spectral extraction errors due to night sky line contamination or low signal to noise is available, and should be used instead. The data prefactor $k(\lambda | \vN)$ is a systematic error term that aims to account for errors in the blaze-correction or flux-calibration. $\vN = \{c_0, c_1, c_2, c_3, c_4, \ldots\}$ are the parameters that describe this correction, in our case these are a set of 4 Chebyshev polynomial coefficients for each order. This amounts to a ``re-fluxing'' following the techniques of \citet{elh+06}.

\subsection{Nuisance parameters}
In order to simplify our understanding of the effects of the $k(\lambda | \vN)$ term, we now consider its effects on just one order. The first four Chebyshev polynomials are 
\begin{align*}
  T_0(x) &= 1\\
  T_1(x) &= x\\
  T_2(x) &= 2 x^2 - 1\\
  T_3(x) &= 4 x^3 - 3x\\
\end{align*}
In our implementation, we map the full range of a specific order $\lambda \in [\lambda_{\rm min}, \lambda_{\rm max}]$ to $x \in [-1, 1]$ (or we can map pixel number). Then, for a given set of Chebyshev coefficients, $\vN = \{ c_0, c_1, c_2, \ldots, + c_N \}$, we have 

\begin{align}
  k(\lambda | \vN) &= c_0 T_0(\lambda) + c_1 T_1(\lambda) + \ldots + c_N T_N(\lambda) \\
  k(\lambda | \vN) &= \sum^N_{i = 0} c_i T_i(\lambda)
\end{align}

using this, we can rewrite Equation~\ref{eqn:lnprob} as 
\begin{equation}
  \ln \bigl (p(\vt | \vD) \bigr) \propto - \frac{1}{2} \sum_\lambda \left [\frac{\fM(\lambda | \vstar) - \bigl [ \sum^N_{i = 0} c_i T_i(\lambda) \bigr ] \fD(\lambda) }{\sigma(\lambda)} \right ]^2
  \label{eqn:lnprob2}
\end{equation}
To simplify the following discussion, we consider just one wavelength $\lambda$ and then generalize this to a sum over $\lambda$ later. If we expand out the square in Equation~\ref{eqn:lnprob2}, then for a given $\lambda_i$ we have
\begin{equation}
 \ln \bigl (p(\vt | \lambda_i) \bigr) \propto -\frac{1}{2 \sigma^2} \bigl [ \fM^2 - 2 \fM \fD \sum_n c_n T_n + \fD^2 \sum_n \sum_m c_n T_n c_m T_m \bigr ]
 \label{eqn:lnprob_lambda}
 \end{equation}
where $\sigma$, $\fM$, $\fD$, and $T_n$ are all evaluated at $\lambda_i$

To simplify the math, we can rewrite the Chebyshev coefficients and polynomials as column vectors $\vc$ and $\vec{T}(\lambda)$, respectively
\begin{equation}
  \vc = 
  \begin{bmatrix}
    c_0\\
    c_1\\
    \vdots\\
    c_N
  \end{bmatrix}
\quad\quad\quad
\vec{T}(\lambda) = 
\begin{bmatrix}
T_0(\lambda)\\
T_1(\lambda)\\
\vdots\\
T_N(\lambda)\\
\end{bmatrix}
\end{equation}

then 
\begin{equation}
  k(\lambda | \vN) = \sum^N_{i = 0} c_i T_i(\lambda) = \vec{T}^\trans(\lambda) \cdot \vc
\end{equation}
If we let 
\begin{equation}
  {\bm W}(\lambda) = \vec{T} \cdot \vec{T}^\trans = 
  \begin{bmatrix}
T_0 T_0 & T_0 T_1 &  \hdots & T_0 T_N \\
T_1 T_0 & T_1 T_1 &  \hdots & T_1 T_N \\
\vdots  & \vdots  &  \ddots & \vdots \\
T_N T_0 & T_N T_1 &  \hdots & T_N T_N \\
  \end{bmatrix}
\end{equation}
then we can rewrite 
\begin{equation}
  \sum_n \sum_m c_n T_n c_m T_m = \vc^\trans \cdot {\bm W} \cdot \vc
\end{equation}
and we can rewrite Equation~\ref{eqn:lnprob_lambda} as 
\begin{equation}
  \ln \bigl (p(\vt | \lambda_i) \bigr) \propto -\frac{1}{2 \sigma^2} \bigl [ \fM^2 - 2 \fM \fD\, \vec{T}^\trans \vc \; + \fD^2 \, \vc^\trans {\bm W}  \vc \bigr ]
\end{equation}

Because matrix multiplication is associative, we can sum ${\bm W(\lambda)}$ and $\vec{T}^\trans(\lambda)$ across all $\lambda$, and rewrite
\begin{align}
  {\bm A} &= \sum_\lambda \frac{\fD(\lambda)}{\sigma^2(\lambda)} {\bm W}(\lambda)\\
  \vec{B}^\trans(\vstar) &= \sum_\lambda \frac{\fM(\lambda | \vstar) \fD(\lambda)}{\sigma^2(\lambda)} \vec{T}^\trans(\lambda)\\
  g(\vstar) &= -\frac{1}{2} \sum_\lambda \frac{\fM^2(\lambda|\vstar)}{\sigma^2(\lambda)}\\
\end{align}
now we can rewrite Equation~\ref{eqn:lnprob2} as 
\begin{equation}
  \ln \bigl (p(\vstar, \vN | \vD) \bigr) \propto - \frac{1}{2} \vc^\trans {\bm A} \vc + \vec{B}^\trans(\vstar) \vc + g(\vstar)
  \label{eqn:lnprob_matrix}
\end{equation}
where remember $\vN = \{c_0, c_1, \ldots, c_N \}$. Now
\begin{equation}
  p(\vstar, \vN | \vD) \propto \exp \left ( - \frac{1}{2} \vc^\trans {\bm A} \vc + \vec{B}^\trans(\vstar) \vc + g(\vstar) \right )
  \label{eqn:posterior_vec}
\end{equation}
The parameters we are most interested in are $\vstar$, so we can analytically marginalize out the nuisance parameters
\begin{equation}
  p(\vstar | \vD) = \int p(\vstar, \vN | \vD) \dd \vN
\end{equation}
this is a multi-dimensional Gaussian, whose analytic integral we use from (Wikipedia, Abramowicz and Stegun)
\begin{align}
  p(\vstar | \vD) &= \int \exp \left ( - \frac{1}{2} \vc^\trans {\bm A} \vc + \vec{B}^\trans(\vstar) \vc + g(\vstar) \right ) \dd \vc\\
  p(\vstar | \vD) &= \sqrt{\frac{(2 \pi )^N}{ {\rm det} \bigl |{\bm A} \bigr |}} \exp{ \left ( \frac{1}{2} \vec{B}^\trans {\bm A}^{-1} \vec{B} + g \right )}
\end{align}
which means that for any set of parameters $\vstar$, only $\vec{B}$ and $g$ need to be evaluated, ${\bm A}$ can be computed once and stored.

\subsection{Including Gaussian priors on nuisance parameters}
In order to constrain the amount of ``re-fluxing'' to be consistent with an independent determination of the systematic error floor in flux-calibration (for example, 20\%), we can add Gaussian priors that limit each polynomial to within a certain amount. For example, we can limit the multiplicative linear term $c_1$ to within an amplitude of 0.2.

This corresponds to 
\begin{equation}
  p(\vN) \propto \exp \left ( -\frac{1}{2} (\vc - \vec{\mu})^\trans {\bm D} (\vc - \vec{\mu}) \right )
\end{equation}
where 
\begin{equation}
  {\bm D} = 
  \begin{bmatrix}
    \sigma_0^{-2} & 0 & \hdots & 0 \\
    0 & \sigma_1^{-2} & \hdots & 0 \\
    \vdots & \vdots & \ddots & 0 \\
    0 & 0 & \hdots & \sigma_N^{-2} \\
  \end{bmatrix}
\end{equation}
where $\sigma_0$ represents the width of the Gaussian prior on the zeroth Chebyshev coefficient. Then revisiting Equation~\ref{eqn:posterior} as
\begin{equation}
  p(\vstar, \vN | \vD) \propto p( \vD | \vstar, \vN) p(\vstar) p(\vN)
\end{equation}
we write Equation~\ref{eqn:posterior_vec} as 
\begin{equation}
  p(\vstar, \vN | \vD) \propto \exp \left ( - \frac{1}{2} \vc^\trans {\bm A} \vc + \vec{B}^\trans(\vstar) \vc + g(\vstar) \right )  \exp \left ( -\frac{1}{2} (\vc - \vec{\mu})^\trans {\bm D} (\vc - \vec{\mu}) \right ) p(\vstar)
  \label{eqn:posterior_prior}
\end{equation}
we can expand 
\begin{align}
  -\frac{1}{2} (\vc - \vec{\mu})^\trans {\bm D} (\vc - \vec{\mu}) &= \frac{1}{2}\left ( -\vc^\trans {\bm D} \vc + \vc^\trans {\bm D} \vec{\mu} + \vec{\mu}^\trans {\bm D} \vc - \vec{\mu}^\trans {\bm D} \vec{\mu} \right )\\
  &= -\frac{1}{2} \vc^\trans {\bm D} \vc + ({\bm D} \vec{\mu})^\trans \vc - \frac{1}{2} \vec{\mu}^\trans {\bm D} \vec{\mu}
\end{align}



\acknowledgments
IC would like to graciously acknowledge Gregory Green, Doug Finkbeiner, and Daniel Eisenstein for many fruitful discussions about statistics and spectroscopy. 

%\appendix
%
%\section{}
%
%\begin{deluxetable}{ll}
%\tablecaption{\label{table:} Title}
%\tablehead{\colhead{Col1} & \colhead{Col2}}
%\startdata
%\enddata
%\tablecomments{}
%\end{deluxetable}

\bibliography{disks}
\bibliographystyle{hapj}
\end{document}


