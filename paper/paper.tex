\documentclass[preprint]{aastex} %double-column, single-spaced document:
%\documentclass[iop,floatfix]{emulateapj} 

\usepackage{hyperref}
%\usepackage{graphicx}
%\usepackage{apjfonts}
\usepackage{enumerate}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{bm}

\newcommand{\prob}{{\rm prob}}
\newcommand{\qN}{\{q_i\}_{i=1}^N}
\newcommand{\yN}{\{y_i\}_{i=1}^N}
\newcommand{\vt}{\vec{\theta}}
\newcommand{\vg}{\vt_{\star, {\rm grid}}}
\newcommand{\vstar}{\vt_{\star}}
\newcommand{\vN}{\vt_{\rm N}}
\newcommand{\vc}{\vec{c}}
\newcommand{\fM}{f_{\rm M}}
\newcommand{\fD}{f_{\rm D}}
\newcommand{\vD}{\vec{D}}
\newcommand{\dd}{\,{\rm d}}
\newcommand{\trans}{\mathsf{T}}
\newcommand{\Z}{[{\rm Fe}/{\rm H}]}
\newcommand{\A}{[\alpha/{\rm Fe}]}

%\slugcomment{}
%\shorttitle{}
%\shortauthors{}

\begin{document}

\title{A method for inference of fundamental stellar parameters using high resolution spectra}
\author{\today{}\\
\medskip
Ian~Czekala\altaffilmark{1} et al.
%Author2\altaffilmark{2},
}

\altaffiltext{1}{Harvard-Smithsonian Center for Astrophysics, 60 Garden Street MS 10, Cambridge, MA 02138}
%\altaffiltext{2}{Institution 2}
\email{iczekala@cfa.harvard.edu}

\begin{abstract}
  Matching observed spectra of stars against a grid synthetic spectra is a popular approach to determine fundamental stellar parameters such as effective temperature, surface gravity, and metallicity. However, a fundamental problem with this approach is that the synthetic spectra contain systematic errors such as wrong or missing spectral lines. In the presence of such errors, a simple $\chi^2$ fit between spectra will likely yield biased parameters and incorrect uncertainty estimates. This behaviour is analogous to the way a few strong outliers might severely bias the fit to a straight line. These spectral outliers are difficult to identify \emph{a priori}; procedures such as sigma clipping might be prone to their own systematic effects and discard useful information. We outline an approach to probabilistically identify and account for spectral features that deviate from reality. Such an approach is most needed when operating at high spectral resolution ($R >$ 10,000), since this is when systematic deviations begin to appear most striking, however our technique is still valid for lower resolution spectra as well. We also outline some approaches for spectral fits to brown dwarfs and planets, where instead of spectral lines there may be systematic deviations of a larger swath of the spectrum.
\end{abstract}

%\keywords{}

%\begin{figure}[htb]
%\begin{center}
%\includegraphics{file}
%\caption{}
%\label{fig:}
%\end{center}
%\end{figure}

\section{Introduction and review of existing methods}

The fundamental parameters of stars---mass, luminosity, effective photospheric temperature, surface gravity, and metallicity--are of supreme interest to astrophysicists for a wide variety of reasons. A salient example is that most newly discovered exoplanets must necessarily their mass and radius measured relative to the properties of their host stars (e.g. \citealt{tfs+12,blj+12,ssm+13}). The properties of stars are of course also important in their own right because they inform us about our ideas of stellar evolution \citep{dm97, bca+02}. Because stars are the fundamental units of the cosmos, techniques to infer the properties of stars date back to the dawn of astronomy. We briefly review some of the modern and popular techniques in the literature. Approaches can be broadly classified into techniques that compare stars by some broadly defined indices and techniques that attempt to forward-model an observed stellar spectrum by doing a pixel-to-pixel comparison.

\subsection{Classification techniques}
Many of existing techniques were written to analyze low to medium resolution ($R < $ 10,000) spectra. I am calling them similarity techniques because they focus on determining and recognizing patterns of spectra useful in classification.

\paragraph{Line indexes} By cataloguing the equivalent widths of chosen spectral lines into indexes, one can correlate the index with stellar properties and identify features that are sensitive temperature, surface gravity, or metallicity. Then, one matches the indecies of a new star against these relationships and derives a set of stellar parameters. A recent example using M dwarfs is \citet{nci+14}. This technique works, but there are many things that could be improved. First, the selection of which lines to measure is arduous and based upon expert knowledge of previously catalogued stars. This is both a disadvantage and an advantage. On one hand, by choosing a subset of lines to fit, one may be unwittingly throwing away useful information. On the other hand, it allows the modeller to discard regions of the spectrum that do not follow a simple trend with stellar parameters. Second, while measuring equivalent widths is easy to do, this discards any useful information contained in the shape of the line profile. 

\paragraph{Principal component analysis} PCA reduces the dimensionality of the data space and searches for the most salient features of the spectra \citep{ptb14}. How does one properly estimate parameter uncertainties in such an approach? It seems like this approach exists because it is an easy thing to do.

Matrix inversions: MATISSE \citep{rbd06}. This method is similar to PCA in that it derives basis vectors that describe the fundamental parameters of the star. In the PCA analysis, the principal components do not necessarily correspond to true physical quantities. The MATISSE algorithm determines weights that ensure the components are similar to the stellar parameters. They only test the algorithm against input synthetic stellar spectra to detect biases in the parameters, and against the Sun and Arcturus. They do acknowledge that there will be systematic deviations between the synthetic spectra and real spectra, but they do not account for the possible inflation of error estimates.

Given the inherent systematic uncertainties involved in each method, some groups elect to combine the inference from different algorithms, such as (SLOAN SSPP) \citep{lbs+08}. This approach includes machine learning along with many of the previously mentioned techniques.

\paragraph{Machine learning}
Many machine learning techniques exist and classify spectra by comparing patterns between observed spectra and a training set of spectra (which could be real spectra of stars classified by an expert or a synthetic library that has known $\vg$). However, my first impression is that these tools are designed to handle large collections of spectra from stellar surveys such as SDSS \citep{sam+10}. In our immediate project involving T Tauri stars, we only have a few ($\lesssim 20$) stars that we wish to study in exquisite detail, incorporating specific parameters such as $v \sin i$, extinction, and veiling. However, I am a novice when it comes to machine learning so I may be missing something here.

\subsection{Pixel to pixel techniques using synthetic spectral libraries}
\label{sec:pix}
Many high quality synthetic spectral grids are now available. Nearly all synthetic stellar libraries are parameterized by the ``fundamental'' stellar parameters of effective temperature, surface gravity, metallicity, and alpha enhancement. We define this combination as 
\begin{equation}
  \vg = \{T_{\rm eff}, \log(g), \Z, \A \}
\end{equation}
Naively, it makes sense to compare an observed spectrum against spectra from the synthetic grid. Using some goodness of fit statistic one can identify the $\vg$ that best reproduce the observed spectrum. 

The pixel-to-pixel likelihood function will evaluate the residuals in some form of 
\begin{equation}
  {\rm residual} = \frac{ {\rm data} - {\rm model}}{ {\rm noise}}
\end{equation}
where the best model is defined as one that minimizes the residuals in some sense. We will discuss other goodness of fit functions soon. Each of these quantities are functions of wavelength $\lambda$, so moving forward we will refer to them as the data spectrum $\fD(\lambda)$, the model spectrum $\fM(\lambda | \vt)$, the noise spectrum $\sigma(\lambda)$, and the residual spectrum $R(\lambda | \vt)$.

Most pixel-to-pixel techniques use a $\chi^2$ likelihood function and seek to minimize the square of the residuals
\begin{equation}
  \chi^2 = \sum_\lambda \left [\frac{ \fD(\lambda) - \fM(\lambda | \vstar) }{\sigma(\lambda)} \right ]^2
\end{equation}
It appears that for many techniques, $\sigma(\lambda)$ is the Poisson photon counting errors. For some data reduction pipelines, a per-pixel ``sigma spectrum'' that accounts for spectral extraction errors due to night sky line contamination or low signal to noise is available, and should be used instead. For more discussion of the noise sources beyond statistical Poisson errors, see \S\ref{sec:residuals}.

\paragraph{ULySS} ULySS \citep{kpb+09} is a mature package to make comparisons for individual spectra as well as unresolved stellar populations. Any spectral library can be used as a backend, including empirical spectral libraries such as the one from ELODIE \citep{psk+07}. They explicitly take into account some of the necessary procedures when fitting spectra, such as continuum-multiplication by a polynomial to account for flux calibration errors and rejection of bad data or model points. They use an iterative kappa-sigma clipping method to discard ``bad'' points for the fit. They test that their clipping technique converges to the same answer by starting guesses located at different parameters $\vt$ and seeing if similar lines are masked and similar best fit parameters $\vt$ are obtained.

\paragraph{\citet{mga13}} fit BT-Settl PHOENIX models to M dwarfs. Because high quality, high resolution M dwarf spectra are difficult to obtain owing to their intrinsic faintness, the development of stellar libraries for later spectral types has typically lagged behind. They use a $\chi^2$-like likelihood function and iterate to mask bad pixels. Unlike ULySS, however, they iterate the method against a sample of $\sim 20$ M dwarfs and track which pixels consistently have the high residuals (median residual amount $> 10$\%, or ten times their measurement error) across all stars. Presumably, such a consistent residual would be due to an incorrect physical constant for a certain line or molecular feature. After masking, they repeat the fitting and claim that after two iterations the masks do not change. They note that in the future, rather than selecting good and bad pixels with a binary mask, ``a more nuanced weighting scheme would be to weight each interval (pixel) according to how consistent it is with the models, or test different weighting schemes to see which gives the best agreement with the bolometric temperatures. \textbf{However regions with modest agreement between the real and synthetic spectra may contain more temperature information than regions with slightly better matches, and we have no a priori information about what spectral regions are the most temperature sensitive.}'' Thus, masking ``bad regions'' could very well destroy stellar information in the pursuit of a better ``fit.'' This is a nuanced point that masking schemes generally neglect. We aim to address later in this document.

\paragraph{``Bayesian'' methods} do exist. \citet{sdm+07} developed a hierarchical Bayesian approach to compare infrared spectra at low resolution ($R \sim $ 1,200) to a synthetic grid in order to determine stellar parameters $\vg$. Their model is hierarchical in that it has parameters describing the statistical and systematic uncertainties of the observed spectrum. The systematic errors are due to the imperfect performance of the instrument, for example a bad flux calibration. They use an ``empirical'' Bayesian model for the noise, meaning that they use data to estimate the prior parameters for the noise \citep{gcs+04}. They use a $\chi^2$ likelihood function and allow for ``shrinkage'' of their hierarchical parameters if there is good agreement between observed and synthetic spectra. Disappointingly, they do explicitly account for systematic errors in the synthesis of actual lines, for example due to wrong atomic constants. One could imagine having a systematic error parameter which describes the scatter in the relationship between $\fM(\lambda | \vg)$ and the ``true'' physical spectrum in the absence of noise $\fD$. Nor do they account for any systematic effect that likely exists between $\vg$ and the true physical stellar parameters (as far as $T_{\rm eff}$ has any physical significance) and is a function of which synthetic grid they use\footnote{For example, in our tests with Kurucz vs. PHOENIX spectra we often find a systematic offset in temperature of $\gtrsim 200$ K.}. However, they do account for interpolation errors by testing interpolating spectra in the pixel space and atmospheric interpolations. Then, they select the best model using squared error loss function and a penalty measure for model complexity.

\citet{sb13} present a comprehensive Bayesian method that can fit low or high resolution spectra in addition to photometry and parallax data. They too, mention that they use a $\chi^2$ likelihood. They mention using special masks for the spectra that block noisy and uninformative regions of the spectrum, however they do not go into further detail about how these masks were created or applied. Their unpublished stellar library focuses on parts of the optical spectrum which contain the most stellar information, as determined by experts. As noted by \citet{sb13}, the grids ``sample the wavelength windows around the spectral features important for diagnostic of FGKM stars: 3850 - 4050\AA\ (Ca I lines), 4350 - 4450\AA\ (G-band, CN sensitive), 4600 - 4900\AA\ (H$\beta$), 5100 - 5300\AA\ (Mg I triplet, main gravity diagnostics), 6400 - 6640\AA\ (H$\alpha$), 8400 - 8800\AA\ (Ca II triplet, also used in \emph{Gaia} and in RAVE stellar survey).'' 


\paragraph{SPC} A workhorse method behind the Kepler follow-up program is Stellar Parameter Classification (SPC) \citep{blj+12}). This compares high resolution optical echelle spectra ($R \sim$ 45,000) to high resolution synthetic grids computed with Kurucz models by a cross-correlation method. The best fit parameters and uncertainties are calculated by the relative values of the cross-correlation coefficient. It is uncertain how statistically justified this uncertainty estimate is. Currently SPC uses only a small region of the optical spectrum ($5000 - 5300$\AA) where the spectral library has been carefully tweaked\footnote{changes to line centers and $\log(gf)$ values} to match high resolution models of the Sun and Vega. 

\paragraph{Other methods} \citet{dj03} do not use a Bayesian model, but they do $\chi^2$ fitting to line profiles in the infrared. \citet{bai10} use a forward modelling approach that interpolates a synthetic grid to estimate stellar parameters from low resolution Gaia spectra.

\paragraph{Comparison of synthetic stellar libraries}
Gaia team members are in a mild panic about the upcoming flood of data. \citet{mc07} compare many spectral grids to determine their weaknesses. \citep{svt+11} discuss the range of stellar grids available for Gaia. \citep{cbm+05} is the Coehlo grid. Munari et al 2005, \citep{hwd+13}.

\paragraph{General trends}
Successful pixel-to-pixel methods either mask bad spectral lines or limit themselves to a narrow region of the spectrum, where they can be relatively certain that the spectral library does not contain gross systematical errors (and effectively masking the rest of the spectrum). Cherry picking regions of the spectrum like this evokes similar concerns to those of \citet{mga13}: how do you know that you are fitting the region(s) of the spectrum that deliver the right balance of information about the stellar properties?

\subsection{Line fitting using spectral synthesis}
Direct synthesis methods take a step closer towards stellar parameter estimation in the correct domain. For as large a region of the spectrum as is computationally tractable, these codes synthesize a spectrum for direct comparison with the data. Because this is an expensive operation, one must necessarily choose only a few good lines to fit. 

\textbf{MOOG}\footnote{\url{http://www.as.utexas.edu/~chris/moog.html}} \citep{sne73} and \textbf{SME}\footnote{\url{http://www.stsci.edu/~valenti/sme.html}} \citep{vp96} are two popular synthesis codes. Given a set of stellar parameters, elemental abundances, and oscillator strengths, the program will interpolate a model atmosphere and then synthesize a spectrum (over a small region of wavelength). Then, they use a Levenberg-Marquartd algorithm to converge to the best-fit parameters. Their approach has the benefit of enabling individual tweaks to elemental abundances and oscillator strengths, something that is impossible in the aforementioned approaches.

In order to make the direct synthesis approach computationally tractable, one must use the simplest atmospheric models and radiative transfer. Powerful codes like PHOENIX \citep{hwd+13} \emph{should} have more accurate atmospheres and physics, but they take a long time to run ($\sim$ hours) per model, and the code is not publicly available. 

\subsection{Stellar parameter estimation in a utopia}
The difficulties encountered by the similarity techniques and synthetic grid comparisons are a result of fitting in the wrong parameter space. Yes, the combination of $\vg$ is an excellent proxy to describe a star, but in truth (and at high resolution) each star is slightly different. The ``model'' that is actually producing the synthetic spectrum has many, many parameters that have been collapsed down into $\vg$. These parameters include the atmospheric structure (relations of temperature and pressure as a function of stellar depth), individual elemental abundances (all models quote a value relative to solar ($\Z$) and some synthetic grids include alpha enhancements ($\A$)), and atomic constants describing the opacity contribution of a line. \emph{All} of these parameters have uncertainties in them, some of them quite dramatic (opacities and line lists are the frequently cited), yet for a synthetic grid these parameters are \emph{fixed}. There may still be some missing physics that would be needed to produce a perfect spectrum within the noise (say for example the cores of Ca lines, or dust settling at the latest spectral types), but my naive impression is that the field of stellar spectral synthesis is mature enough that nearly all lines could be properly reproduced given sufficient scrutiny to the atmosphere and atomic constants. 

If we had infinite computational power, a tantalizing approach would be to use a model with stellar parameters $\vg$, atmospheric parameters (such as pressure and temperature profiles), and uncertain atomic constants such as oscillator strengths. One could fit for individual elemental abundances and marginalize over the atomic constants. This would extract a tremendous amount of information from a high resolution spectrum. Unfortunately, the relationship between $\vt$ and the spectrum $\fD(\lambda | \vt)$ is non-linear, meaning that a new spectrum would need to be synthesized for each new $\vt$. It may be possible to make an approximate a linear relationship between say an oscillator strength and the line depth as long as the tweak is minor. If no reasonable approximation can be found, then the radiative transfer becomes an insurmountable bottleneck.

If the approximations could reduce the computation of synthetic spectrum, efficient MCMC sampling parallelized on a cluster might put this utopian vision within reach in the near future. However, without a collaborator with expert knowledge of spectral synthesis, I do not have the expertise to take such an approach. Many high quality synthetic spectral grids do exist, and so our current efforts are focused on addressing the shortfalls of the previously mentioned pixel-to-pixel methods.

\section{General problems with the spectral library approach}

Forward modelling approaches (pixel-to-pixel) have the potential to be more powerful than other stellar parameter estimation techniques. When we model directly to the data space, we have the ability to account for instrumental and systematic effects, allowing us to derive a robust estimate of our uncertainty by marginalizing over the nuisance parameters. We can also derive more complicated models to fit unresolved binary or multiple star systems. In fact, many of the most interesting systems for which we might want to determine stellar parameters are in multiple systems since we can dynamically determine masses for these systems and thus compare to evolutionary models.  

As discussed in \S\ref{sec:pix}, many forward modelling approaches exist, yet nearly all use a $\chi^2$ likelihood function and most employ some sort of masking scheme. The more successful pixel-to-pixel methods use low or moderate resolution spectra ($R \sim$ 1,000 to 10,000) or a grid over a narrow range of wavelength space with finely tuned atomic constants, eschewing difficult decisions about what to mask. \textbf{The true fundamental problem with a pixel-to-pixel comparison is that the stellar spectra are not a perfect representation of reality}. This may be an obvious statement, but systematic errors in the synthetic spectra means that the previously valid assumption of Poisson errors and a $\chi^2$ likelihood function breaks down. In order to derive unbiased parameter estimates, systematic errors must be identified and properly handled. 

\begin{figure}[!htb]
\begin{center}
\includegraphics{lineclasses}
\caption{The four classes of line behavior, from left to right: \textbf{Class 0}: the model behaves as expected. \textbf{Class I}: a spectral line is present in the data but missing from the model (in this case, the line is also blended). \textbf{Class II}: a spectral line exists in the model where there is only continuum in the data. \textbf{Class III}: lines exist in both the data and model, however they are of the wrong strength. Some lines are too strong while others are too weak. The lower panels show the residual spectra in units of the observational uncertainty (Poisson uncertainty only).}
\label{fig:lineclasses}
\end{center}
\end{figure}

\subsection{Classes of synthetic spectral line behavior}
From our forward modelling experiments using the modified Kurucz grid used in \citet{blj+12} and the recent PHOENIX grid \citep{hwd+13}, we have identified ways that synthetic spectra diverge from the our observed data. To start, we will describe these classes of behaviour as being tied to specific spectral lines present against a well-defined continuum. Later on, we will use the classes to describe features more generally to cover the behavior of molecular features in later type spectra. See Figure~\ref{fig:lineclasses} for visual examples of each class of behavior.

\paragraph{Class 0} These lines actually produce a good fit with a residual nearly consistent with the Poisson counting uncertainty (see \S\ref{sec:TRES} for how the data is processed). If every region of the spectrum behaved like this, we might be justified in using a $\chi^2$ likelihood function and calling it a day.

\paragraph{Class I} A spectral line appears in the data but not in the model spectrum over a wide range of probable stellar parameters (1000 K in $T_{\rm eff}$, and 1 dex in $\log(g)$, $\Z$, and $\A$. Such a type of error is perhaps the easiest to understand and to forgive. For example, such a line could easily appear in the data due to interstellar absorption or telluric absorption, something which a stellar model could never produce. The model could also fail to reproduce a true stellar line due to a missing opacity source.

\paragraph{Class II} A spectral line appears in the model but not in the data over a wide range of probable stellar parameters (1000 K in $T_{\rm eff}$, and 1 dex in $\log(g)$, $\Z$, and $\A$. This is a more puzzling error, because presumably atomic constants would be much more likely to be \emph{missing} than added in by accident. Presumably this might be a combination of incorrect atomic data coupled with incorrect atmospheric structure that incorrectly excites a line.

The traditional way to account for Class I and Class II errors is to sigma clip the bad lines in the spectrum \citep{kpb+09, mga13}. A region of the spectrum with Class I or II errors is useless for determining spectral parameters. While sigma-clipping routines are to be avoided, if they can correctly mask these regions, then at least they are moving in the right direction.

There is additional information available to help determine which regions of the spectrum suffer from Class I or II errors.For a normal star with absorption lines against a continuum, Class I residuals will always be negative and not change significantly with $\vg$, since we the model can only produce continuum in that region. Class II residuals will always be positive and their magnitude should decrease with decreasing metallicity, since lower metallicity will decrease the strength of all lines. We could easily store the partial derivatives of an individual synthetic spectrum with respect to $\vg$
\begin{equation}
  \bigl \{ \frac{\partial \fM(\lambda | \vg)}{ \partial T_{\rm eff}}, \frac{\partial \fM(\lambda | \vg)}{ \partial \log(g)}, \frac{\partial \fM(\lambda | \vg)}{ \partial \Z}, \frac{\partial \fM(\lambda | \vg)}{ \partial \A} \bigr \}
  \label{eqn:derivative}
\end{equation}
and coupled with a prior on the range of parameters over which the residuals will be correlated, we could shore up the classification of Class I and II errors. 

\paragraph{Class III} errors occur when the line strengths between the data and model do not match at a $\vg$ that fits a majority of the other lines. What makes Class III errors truly pernicious is that for a specific line, there does exist a set of $\vg$ that could properly fit the line, though this parameter combination would result in bad fits for the majority of the other lines. Class I and II errors are easy to identify since the lines will always be bad across a wide range of $\vg$ and have predictable behaviors with change in $\vg$. Class III errors, however, come in both directions: the line can be too strong or too weak. Identifying Class III errors requires making a choice about which lines to trust and how much. 

Most of the Class III errors can probably be identified by finding the regions of the spectrum that have the highest residuals after dealing with Class I and II errors, although some mildly incorrect Class III lines will be difficult to distinguish from correct (Class 0) lines that are simply fit with a less probable $\vg$. Once identified, properly handling Class III errors requires careful thought because these regions of the spectrum \emph{still contain information about $\vg$}. If we take an masking approach as with Class I and II errors, we will be throwing it away. It might even be the case that Class III regions contain \emph{the most} information about $\vg$, since these ``delicate regions'' are the most sensitive to changes in $\vg$ at that particular $\vg$ and thus are tricky to tune correctly. In other words, Class III regions might also be regions where the spectrum derivative (Eqn~\ref{eqn:derivative}) is largest.

To summarize, the fundamental problem with comparing high resolution stellar spectra to grids of synthetic stellar spectra is that no one model $\vg$ will fit all of the lines properly. One can either limit their fits to regions of the spectrum that they know their spectral libraries to be accurate (implicitly masking the rest and potentially discarding a large amount of information) or suck it up and forward model the regions where the models perform poorly.


\section{Instrumental effects and ``nuisance'' parameters in the limit of a perfect stellar model}
\label{sec:perfect}
While this document aims to be general in scope, any forward-modelling approach must also discuss the characteristics of the instrument that performed the observations. We are currently modelling data obtained with the \emph{TRES}\footnote{\url{http://tdc-www.harvard.edu/instruments/tres/}} echelle spectrograph, and so we will describe the characteristics of the instrument and the calibration approaches we have taken, although we note that our approach is easily generalized for data obtained with other spectrographs.

\emph{TRES} is a 51-order echelle spectrograph on the 1.5-m Tillinghast reflector at Whipple Observatory. It spans the full optical range ($3800-9000$\AA) at a resolution of $R=48,000$ ($6.8\;{\rm km/s}$). Echelle spectrographs impart a strong blaze function upon each order, where transmission is largest in the center of the order and declines rapidly as one moves towards the edge of the order. The \emph{TRES} pipeline extracts orders such that they span a wavelength region until the transmission reaches $\lesssim$ 25\% of the central transmission in that order. Each order spans $\sim 100$\AA\ and the blue orders overlap while there are gaps between the wavelength coverage of the red orders. This spectra are blaze-corrected by dividing by a blaze function determined from fitting nightly exposures of flat lamps. Then, the spectrum is flux calibrated using sensitivity functions determined from nightly observations of spectrophotometric standards (O stars). The flux-calibrated spectrum is in units of ergs/s/${\rm cm}^2$/\AA.

\paragraph{Processing TRES Data}
\label{sec:TRES}
Because the \emph{TRES} reduction pipeline does not produce a ``sigma spectrum'' like that determined by a standard IRAF reduction, we instead calculate the Poisson uncertainty on each pixel. We can find the signal-to-noise ratio of each pixel using the raw (still-blazed) spectrum, which is in units of ``counts.''
\begin{equation}
  [S/N](\lambda) = \sqrt{ {\rm cts}(\lambda)}
\end{equation}
the Noise-to-Signal ratio is the inverse 
\begin{equation}
  [N/S](\lambda) = \frac{1}{\sqrt{ {\rm cts}(\lambda)}}
\end{equation}
To determine the uncertainty of the flux-calibrated spectrum, we multiply by N/S. The $[N/S](\lambda)$ ``spectrum'' is larger at the edges of each order due to the reduced transmission. Please note that statistic pixel uncertainties derived in this manner neglect regions of the spectrum that might have a higher variance due to sitting atop a night sky line, but short of implementing our own reduction pipeline for \emph{TRES}, there isn't much more we can do at the moment.

\paragraph{Generating a model spectrum} We parameterize the fit to a star by $\vt = \{\vstar, \vN \}$, where stellar parameters include $\vstar = \{T_{\rm eff}, \log(g), \Z, \A, v \sin i, v_z, A_v, R_\star^2/d^2 \}$. The data vector $\vD = \fD(\lambda)$ represents the \emph{TRES} spectrum. To generate a model spectrum given a set of parameters $\fM(\vt)$, a model spectrum is created by first linearly interpolating in $T_{\rm eff}, \log(g), \Z, \A$ space between grid points of instrumentally broadened and oversampled synthetic stellar spectra. Then, the spectrum is rotationally broadened for $v \sin i$ using Fourier techniques, Doppler shifted by $v_z$, extinction corrected for $A_V$, multiplied by $R_\star^2/d^2$ for a solid angle correction, and finally downsampled to the exact pixels of the TRES spectrum using spline interpolation\footnote{Because we are dealing with flux-calibrated data and synthetic spectra, we \emph{resample} to lower resolution. We do not \emph{rebin} because we are dealing with flux-density, not photon counts}.

FIGURE: showing the high resolution, raw $R =$ 500,000 spectrum compared to the one convolved with a kernel down to $R =$ 50,000. This shows that lines are blended, but it is important to keep in mind that we do have access to the high resolution models when talking about bad pixels or bad line-species. For example, we could inject bad lines in this high resolution space.

\subsection{Applied to a Gaussian likelihood function}
For the rest of \S\ref{sec:perfect}, we proceed as if there were no Class I, II, or III errors present in our model so that we may illustrate our technique for dealing with uncertainties in the flux-calibration. Although per-pixel photon counting errors are Poisson, we can safely assume that we have enough counts that we are in the Gaussian limit, although see the discussion in \S\ref{sec:residuals}. Then our likelihood function becomes a pixel-by-pixel $\chi^2$ comparison between the data spectrum $\fD$, and the model spectrum $\fM$, summed over the wavelength axis. 
\begin{equation}
 p(\vD | \vt) = {\cal L} \propto \exp \left(-\frac{\chi^2}{2} \right)
\end{equation}
\begin{equation}
  \chi^2 = \sum_\lambda \left [\frac{ \fD(\lambda) - k(\lambda | \vN) \fM(\lambda | \vstar) }{\sigma(\lambda)} \right ]^2
\end{equation}
With flat priors on $\vstar$ and $\vN$ (an assumption we will later relax) the logarithm of our posterior probability function is then
\begin{equation}
  \ln \bigl [p(\vt | \vD) \bigr] \propto - \frac{1}{2} \sum_\lambda \left [\frac{ \fD(\lambda) - k(\lambda | \vN) \fM(\lambda | \vstar) }{\sigma(\lambda)} \right ]^2
  \label{eqn:lnprob}
\end{equation}

The prefactor $k(\lambda | \vN)$ is a calibration function that aims to account for errors in the blaze-correction or flux-calibration by multiplying the synthetic spectrum by a polynomial. The parameters describing this function are the coefficients of the polynomial. This approach of accounting for large-scale calibration errors in the spectrum is more justified than the traditional approach of normalizing to the stellar continuum. From a flux-calibration experiment where we observed spectrophotometric standards BD+28 and BD+25 multiple times, we calibrated the dispersion in (FIGURE showing the sensitivity function for a few representative orders) flux-calibration function, or IRAF's ``sensitivity function.'' These tests revealed that the largest source of error in flux-calibration was actually in the overall magnitude of individual order, this could be due to slit losses, poor correction due to parallactic angle, or scattered light within the instrument. These corrections are smaller than 10\%. But more importantly, the corrections in the higher order terms (ie, the shape of the bandpass calibration for each order) are small ($\lesssim 3\%$). As described in \S\ref{sec:priors}, we can set priors on these coefficients. This prior information on the calibration parameters motivates our choice of functional parameterization for the nuisance parameters in \S\ref{sec:nuisance}. The polynomial approach is also better than the normalization approach if the order only spans a small region of the spectrum. If a M star absorption band spanned the whole order, one might accidentally normalize to the feature rather than a pseudo-continuum.


\subsection{Nuisance parameters}
\label{sec:nuisance}
In our case, $\vN = \{c_0, c_1, \ldots, c_N\}$ are a set of 4 Chebyshev polynomial coefficients for each order. This amounts to a ``re-fluxing'' of the model following the techniques of \citet{elh+06}. We apply the re-fluxing to the model rather than attempt to re-flux the data since this operation would also need to re-scale the noise, making $\chi^2$ a complicated function of $k$.


In order to understand the effects of the $k(\lambda | \vN)$ term, we now consider its effects on just one order of the 51-order echelle spectrum. The first four Chebyshev polynomials are 
\begin{align*}
  T_0(x) &= 1\\
  T_1(x) &= x\\
  T_2(x) &= 2 x^2 - 1\\
  T_3(x) &= 4 x^3 - 3x\\
\end{align*}
In our implementation, we map the full pixel range of a specific order $\lambda \in [\lambda_{\rm min}, \lambda_{\rm max}]$ to $x \in [-1, 1]$. By mapping pixel number in a given order, rather than wavelength, we are insensitive to wavelength shifts of the model. Then, for a given set of Chebyshev coefficients, $\vN = \{ c_0, c_1, c_2, \ldots, c_N \}$, we have 
\begin{align}
  k(\lambda | \vN) &= c_0 T_0(\lambda) \left [1 + c_1 T_1(\lambda) + \ldots + c_N T_N(\lambda) \right ]\\
  k(\lambda | \vN) &= c_0 \left [1 + \sum^N_{i = 1} c_i T_i(\lambda) \right ]
  \label{eqn:k_new}
\end{align}
When applied to scale the model flux $\fM$, this becomes
\begin{align}
  k(\lambda | \vN) \fM &=  \left [1 + \sum^N_{i = 1} c_i T_i(\lambda) \right ] (c_0 \fM)\\
  k(\lambda | \vN) \fM &= c_0 \fM + c_1 T_1 (c_0 \fM) + \ldots + c_N T_N (c_0 \fM)
\end{align}

This functional form of $k$ allows $c_0$ to scale by large amounts while the higher order terms (denoted by $c_n$) are perturbations on top of the \emph{scaled} model ($c_0 \fM$). However, see \S\ref{sec:gaussian_simplification} for an alternative model when perturbations are small. We can rewrite Equation~\ref{eqn:lnprob} as 
\begin{equation}
  \ln \bigl [p(\vt | \vD) \bigr] \propto - \frac{1}{2} \sum_\lambda \left [\frac{\fD(\lambda) - \bigl [1 + \sum^N_{i = 1} c_i T_i(\lambda) \bigr ] c_0 \fM(\lambda | \vstar)  }{\sigma(\lambda)} \right ]^2
  \label{eqn:lnprob2}
\end{equation}
To simplify the following discussion, we consider a single wavelength $\lambda_i$ and then generalize this to a sum over $\lambda$ later. If we expand out the square in Equation~\ref{eqn:lnprob2}, then for a given $\lambda_i$ we have
\begin{equation}
  \ln \bigl [p(\vt | \lambda_i) \bigr] \propto -\frac{1}{2 \sigma^2} \left [ \fD^2 - 2 \fD c_0 \fM \left [1+ \sum_{i=1}^N c_i T_i \right ]+ c_0^2 \fM^2 \left [1 + \sum_{i=1}^N c_i T_i \right ]^2 \right ]
 \label{eqn:lnprob_lambda}
 \end{equation}
where $\sigma$, $\fD$, $\fM$, and $T_{n,m}$ are all evaluated at $\lambda_i$. We can further expand this to
\begin{equation}
  \ln \bigl [p(\vt | \lambda_i) \bigr] \propto - \frac{1}{2 \sigma^2} \left [ \fD^2 - 2 \fD c_0 \fM \left [1+ \sum_{i=1}^N c_i T_i \right ]+ c_0^2 \fM^2 \left [1 + 2 \sum_{i=1}^N c_i T_i + \sum_{i=1}^N \sum_{j=1}^N c_i T_i c_j T_j \right ] \right ]
  \label{eqn:expanded}
 \end{equation}

To simplify this math, we can rewrite the Chebyshev coefficients and polynomials as column vectors $\vc$ and $\vec{T}(\lambda)$, respectively
\begin{equation}
  \vc = 
  \begin{bmatrix}
    c_1\\
    c_2\\
    \vdots\\
    c_N
  \end{bmatrix}
  \hspace{3cm}
\vec{T}(\lambda) = 
\begin{bmatrix}
T_1(\lambda)\\
T_2(\lambda)\\
\vdots\\
T_N(\lambda)\\
\end{bmatrix}
\end{equation}
then 
\begin{equation}
  k(\lambda | \vN) = c_0 \left [ 1 + \sum^N_{i = 0} c_i T_i \right ] = c_0 \left[ 1+ \vec{T}^\trans \cdot \vc \right]
\end{equation}
If we let 
\begin{equation}
  {\bm W}(\lambda) = \vec{T} \cdot \vec{T}^\trans = 
  \begin{bmatrix}
T_1 T_1 & T_1 T_2 &  \hdots & T_1 T_N \\
T_2 T_1 & T_2 T_2 &  \hdots & T_2 T_N \\
\vdots  & \vdots  &  \ddots & \vdots \\
T_N T_1 & T_N T_2 &  \hdots & T_N T_N \\
  \end{bmatrix}
\end{equation}
then we have
\begin{equation}
  \sum_{i =1}^N \sum_{j=1}^N c_i T_i c_j T_j = \vc^\trans \cdot {\bm W} \cdot \vc
\end{equation}
We can rewrite Equation~\ref{eqn:expanded} quadratically in $\vc$ as
\begin{equation}
  \ln \bigl [p(\vt | \lambda_i) \bigr] \propto - \frac{1}{2 \sigma^2} \left [ \fD^2 - 2 \fD c_0 \fM \left [1+ \sum_{i=1}^N c_i T_i \right ]+ c_0^2 \fM^2 \left [1 + 2 \sum_{i=1}^N c_i T_i + \sum_{i=1}^N \sum_{j=1}^N c_i T_i c_j T_j \right ] \right ]
 \end{equation}
\begin{equation}
  \ln \bigl [p(\vt | \lambda_i) \bigr] \propto - \frac{1}{2} \frac{c_0^2 \fM^2}{\sigma^2} \sum_{i=1}^N \sum_{j=1}^N c_i T_i c_j T_j + \left (\frac{- c_0^2 \fM^2 + c_0 \fD \fM}{\sigma^2} \right ) \sum_{i=1}^N c_i T_i  -  \left( \frac{c_0^2 \fM^2 - 2 c_0 \fD \fM + \fD^2}{2 \sigma^2} \right)
\end{equation}
\begin{equation}
  \ln \bigl [p(\vt | \lambda_i) \bigr] \propto - \frac{1}{2} \frac{c_0^2 \fM^2}{\sigma^2}  \vc^\trans {\bm W} \vc + \left (\frac{- c_0^2 \fM^2 + c_0 \fD \fM}{\sigma^2} \right ) \vec{T}^\trans \vc -  \left( \frac{c_0^2 \fM^2 - 2 c_0 \fD \fM + \fD^2}{2 \sigma^2} \right)
\end{equation}

Because matrix multiplication is associative, we can sum ${\bm W(\lambda)}$ and $\vec{T}^\trans(\lambda)$ across all $\lambda$, and define
\begin{align}
  {\bm A} &= c_0^2 \sum_{\lambda} \frac{\fM^2(\lambda)}{\sigma^2(\lambda)} {\bm W}(\lambda) \\
  \vec{B} &= \sum_{\lambda} \frac{-\fM^2(\lambda) c_0^2 + \fD(\lambda) \fM (\lambda) c_0}{\sigma^2(\lambda)} \vec{T}(\lambda)\\
  g &= -\frac{1}{2} \sum_{\lambda} \frac{c_0^2 \fM^2(\lambda) - 2 c_0 \fD(\lambda) \fM(\lambda) + \fD^2(\lambda)}{\sigma^2(\lambda)} 
\end{align}
Where ${\bm A}$, $\vec{B}$, and $g$ are each a function of $\vstar$, $\vN$, and $\vD$. Now we can rewrite Equation~\ref{eqn:lnprob2} as 
\begin{align}
  \ln \bigl [p(\vstar, \vN | \vD) \bigr] &\propto - \frac{1}{2} \vc^\trans {\bm A} \vc + \vec{B}^\trans \vc + g\\
  p(\vt | \vD) = p(\vstar, \vN | \vD) &\propto \exp \left ( - \frac{1}{2} \vc^\trans {\bm A} \vc + \vec{B}^\trans \vc + g \right )
  \label{eqn:lnprob_matrix}
\end{align}
Here we have the full posterior probability distribution assuming flat priors. This posterior is a function of both the stellar parameters $\vstar$ and the nuisance parameters $\vN$. Generally, we are most interested in the stellar parameters after they have been marginalized over the nuisance parameters. This marginalization encapsulates the uncertainty due to the inference on the nuisance parameters. Because this probability function is a multi-dimensional Gaussian in the higher order Chebyshev coefficients $\vc = \{c_1, c_2, \ldots, c_N \}$, we can do this marginalization analytically, and then sample from the marginalized distribution. 
\begin{equation}
  p(\vstar, c_0 | \vD) = \int p(\vstar, c_0, \vc\; | \vD) \dd \vc
\end{equation}
The analytic multi-dimensional Gaussian integral with linear term \citep{sgd+09} yields 
\begin{align}
  p(\vstar, c_0 | \vD) &\propto \int \exp \left ( - \frac{1}{2} \vc^\trans {\bm A} \vc + \vec{B}^\trans \vc + g \right ) \dd \vc\\
  p(\vstar, c_0 | \vD) &\propto \sqrt{\frac{(2 \pi )^N}{ {\rm det} \bigl |{\bm A} \bigr |}} \exp{ \left ( \frac{1}{2} \vec{B}^\trans {\bm A}^{-1} \vec{B} + g \right )}
\end{align}
Where $N$ is the number of Chebyshev coefficients marginalized over, in our case $N=3$. By doing this marginalization we have reduced the dimensionality of our posterior space from $200+$ parameters ($\sim 7$ stellar parameters plus 3 Chebyshev coefficients for each of 51 echelle orders) to $\sim 60$ ($\sim 7$ stellar parameters and one $c_0$ for each of the 51 echelle orders). This enables a dramatic speedup in the MCMC sampling. For flux-calibrated spectra where the variation in $c_0$ is expected to be small (less than how much?), we can approximate $k$ in a different manner which allows marginalization over the $c_0$ coefficients as well, reducing the dimensionality of the space to only the $\sim 7$ stellar parameters. This approach is detailed in \S\ref{sec:gaussian_simplification}.

\subsection{Including Gaussian priors on nuisance parameters}
\label{sec:priors}
Using this polynomial formalism, it is possible to incorporate prior knowledge about the degree of necessary ``re-fluxing'' through priors on the nuisance parameters. From our flux-calibration experiment, we determined that the overall scaling could range by 10\%, while the higher order corrections were less than 3\%. Because $c_0$ is a scale factor, it should be equally probable to scale up by a factor of two as it is to scale down by a factor of two. This means we should be using a log-normal prior on $c_0$
\begin{align}
  p(c_0) &= \frac{1}{\sqrt{2 \pi} \sigma_{c_0} c_0} \exp \left( -\frac{(\ln c_0)^2}{2 \sigma_{c_0}^2} \right) \\
  \ln p(c_0) &= \ln \left( \frac{1}{\sqrt{2 \pi} \sigma_{c_0} c_0} \right) - \frac{(\ln c_0)^2}{2 \sigma_{c_0}^2}
\end{align}
centered on $c_0 = 1$. Because the higher order polynomial terms are perturbations on the scaled model ($c_0 f$), we assume Gaussian priors centered about 0
\begin{equation}
  p(c_n) = \frac{1}{\sqrt{2 \pi} \sigma_{c_n}} \exp \left( - \frac{c_n^2}{2 \sigma_{c_n^2}} \right)
\end{equation}
which we can express in vector form as
\begin{equation}
  p(\vc\;) \propto \exp \left ( -\frac{1}{2} (\vc - \vec{\mu})^\trans {\bm D} (\vc - \vec{\mu}) \right )
  \label{eqn:nuisance_prior} 
\end{equation}
where 
\begin{equation}
  {\bm D} = 
  \begin{bmatrix}
    \sigma_1^{-2} & 0 & \hdots & 0 \\
    0 & \sigma_2^{-2} & \hdots & 0 \\
    \vdots & \vdots & \ddots & 0 \\
    0 & 0 & \hdots & \sigma_N^{-2} \\
  \end{bmatrix}
\end{equation}
and $\sigma_i$ represents the width of the Gaussian prior on the i-th Chebyshev coefficient, in our case $\sigma_i = \sigma_{c_n} \approx 0.03$. $\vec{\mu}$ is the mean of the Gaussian prior, in nearly all cases we expect to set this to $\vec{\mu} = \{0, 0, \ldots, 0\}$, unless we have prior knowledge that the flux-calibration is systematically warped in one direction. Now, we can write the posterior with priors as
\begin{equation}
  p(\vstar, \vN | \vD) = p(\vstar, c_0, \vc\; | \vD) \propto p( \vD | \vstar, c_0, \vc\; ) p(\vc\;) p(\vstar) p(c_0) 
\end{equation}
\begin{equation}
  p(\vstar, \vN | \vD) \propto \exp \left ( - \frac{1}{2} \vc^\trans {\bm A} \vc + \vec{B}^\trans(\vstar) \vc + g(\vstar) \right )  \exp \left ( -\frac{1}{2} (\vc - \vec{\mu})^\trans {\bm D} (\vc - \vec{\mu}) \right ) p(\vstar) p(c_0)
  \label{eqn:posterior_prior}
\end{equation}
we can expand and rearrange the argument of Equation~\ref{eqn:nuisance_prior} to a similar form that is quadratic in $\vc$
\begin{align}
  -\frac{1}{2} (\vc - \vec{\mu})^\trans {\bm D} (\vc - \vec{\mu}) &= \frac{1}{2}\left ( -\vc^\trans {\bm D} \vc + \vc^\trans {\bm D} \vec{\mu} + \vec{\mu}^\trans {\bm D} \vc - \vec{\mu}^\trans {\bm D} \vec{\mu} \right )\\
  &= -\frac{1}{2} \vc^\trans {\bm D} \vc + ({\bm D} \vec{\mu})^\trans \vc - \frac{1}{2} \vec{\mu}^\trans {\bm D} \vec{\mu}
\end{align}
then we can rewrite
\begin{align}
  {\bm A}^\prime &= {\bm A} + {\bm D}\\
  \vec{B}^\prime &= \vec{B} + ({\bm D} \vec{\mu})^\trans\\
  g^\prime &= g - \frac{1}{2} \vec{\mu}^\trans {\bm D} \vec{\mu} 
\end{align}
in the case that $\vec{\mu} = \{0, 0, \ldots, 0\}$, there is only a non-zero correction to ${\bm A}$. The full posterior probability distribution is then
 \begin{equation}
   \boxed{
  p(\vstar, \vN | \vD) = p(\vstar, c_0, \vc\;) \propto \exp \left ( - \frac{1}{2} \vc^\trans {\bm A}^\prime \vc + \vec{B}^{\prime \trans} \vc + g^\prime \right ) p(\vstar) p(c_0) p(\vc)
}
\end{equation}
This probability function appears in the code as \texttt{lnprob\_lognormal} and has $\sim 7$ stellar dimensions and $\sim 200$ nuisance dimensions.

Using the previous multi-dimensional integral, we can marginalize over the $\vc$ including the Gaussian priors
\begin{equation}
  \boxed{
  p(\vstar, c_0 | \vD) \propto \sqrt{\frac{(2 \pi )^N}{ {\rm det} \bigl |{\bm A}^\prime \bigr |}} \exp{ \left ( \frac{1}{2} \vec{B}^{\prime\trans} {\bm A}^{\prime -1} \vec{B}^\prime + g^\prime \right )} p(\vstar) p(c_0)
}
\end{equation}
where $N$ is the number of Chebyshev coefficients marginalized over, in our case $N=3$. This probability function appears in the code as \texttt{lnprob\_lognormal\_marg} and has $\sim 7$ stellar dimensions and $\sim 50$ nuisance dimensions. If we increase the permissible degree of re-fluxing by increasing $\sigma_i$, this will have the effect of smearing out or widening the posterior on a given $\vstar$.


\subsection{Sampling a un-flux calibrated model}
There also exists a wealth of archival \emph{TRES} data that is not flux calibrated, since its primary purpose was for radial velocity determination of FGK stars. To fit this data we can use outdated sensitivity functions and soften the priors on the coefficients.

This invalidates the previous section on normalization. However, for corrections of less than XX\%, this approximation is better than XX\% accurate.

For the flux-calibration exercise, the overall spread in $c_0$ across all orders should not be more than $\sigma_{c_0} \leq 2$, or the one-sigma deviation of $c_0$ is between $0.125 \leq c_0 \leq 8.0$.

The plot in plots/priortests shows that the log-normal prior is the correct choice. Since we care most about the inference on F, we want to know that the choice on prior of $c_0$ is not biasing the final outcome, while a Gaussian prior might if $\sigma_{c_0}$ was large. How to quantify this error and create a dividing line? For 5\% error? In peak? Do these plots need to be in the paper?
 
Compare distributions by doing a K-S test on the cumulative distributions between Gaussian and Log-normal?

\subsection{Hierarchical modelling}
Can have a $c_1$, etc, that are marginalized over but we sample $c_0$ in a hierarchical manner which informs the overall flux-level.

\subsection{Sampling with emcee}
Citation to DFM \citep{fhl+12}, GW \citep{gw10}, and CosmoHAMMER \citep{asa+13}.


\section{Moving beyond a perfect stellar model}
\label{sec:residuals}


\subsection{Additional sources of noise beyond Poisson}
Presumably we shouldn't have to add an additional source of ``noise'' to the fit, because our nuisance parameters will find an exact flux calibration.

Interpolation errors. Quantified via Husser et al. Ph.D. thesis as well as custom tests.

\subsection{Noise, Masking, Residuals, and Alternative likelihood functions}
In the future, we could modify our likelihood function to use a Student-t statistic instead of a Gaussian, such that it would be more robust in the presence of outlier pixels, however this does not seem necessary at the moment. 

\subsection{Masking}

We must necessarily mask certain wavelength regions of the T Tauri spectra that are contaminated (one might also say ``made more interesting'') by the astrophysical realities of accretion or stellar spots (such as the Balmer lines, which are in emission).  We also accept the assumption that rotation or accretion onto the star does not fundamentally alter the structure of the model atmosphere that generates the model spectrum and that a spectral comparison of the ``clean'' regions of the spectrum is valid.

Section on how to choose masked regions, testing the PHOENIX grid to see where the derivative is highest and thus gives the best leverage on stellar parameters.


Iterative masking to deal with Class I and II errors a la ULySS. They use a $\kappa - \sigma$ routine, where they iteratively sigma clip (Website link).


\section{Gallery of likelihood functions}

If you do a straight up $\chi^2$ fit to the data, you can get posteriors that look like $\pm 1$ K and $\pm 0.01$ dex in gravity and metallicity. This would be better than that obtained by astroseismology! Clearly there is something going wrong here.

Lorentzian, linear combination of Gaussians, Sivia w/ prior on sigma. $\chi^2_Q$ model.

These likelihood functions seem to be an improvement over the straight $\chi^2$ function, in the sense that they give parameter estimates that are 

\subsection{Bad data model}
The bad data model \citep{pre97,dbl10} is a powerful framework for forward-modelling of data, allowing the modeler to account for the probability of outliers.

Now, we have a $q_i$ for each data point, such that for the entire data set we have an ensemble of $\qN$. If this ensemble of $\qN$ is chosen appropriately, and the good data points are flagged as good and the bad data points are flagged as bad, the likelihood will be maximized. Written out explicitly, this is
\begin{multline}
  {\cal L} = p({\bm D} | m,b,\qN,Y_b,V_b) = \prod_{i=1}^N \left [ \frac{1}{\sqrt{2 \pi} \sigma} \exp \left[ - \frac{(y_i - m x_i - b)^2}{2 \sigma^2} \right] \right ]^{q_i}  \times \\
  \left [ \frac{1}{\sqrt{2 \pi (V_b + \sigma_{y_i}^2)}} \exp \left[ - \frac{(y_i - Y_b)^2}{2 (V_b + \sigma_{y_i}^2)} \right]
  \right ]^{1 - q_i}
\end{multline}

using a Bernoulli prior

\begin{equation}
  p(q_i | P_b) = [1-P_b]^{q_i} P_b^{[1-q_i]}
\end{equation}

\begin{equation}
  p(\qN | P_b) = \prod_{i=1}^N [1-P_b]^{q_i} P_b^{[1-q_i]}
\end{equation}

For this data point, we can integrate (in this case, discrete sum) over the two states of $q_i$: $q_i = 1$ and $q_i = 0$. 
\begin{multline}
  p(m, b, P_b, Y_b, V_b | D_i)  =  \int p( m, b, q_i, P_b, Y_b, V_b| D_i) {\rm d} q_i \\
  = p(m, b, q_i=1, P_b, Y_b, V_b | D_i) + p(m, b, q_i=0, P_b, Y_b, V_b | D_i) \\
  = \Bigl[ p_{\rm good}(D_i | m,b)\, p(q_i=1 | P_b) +  p_{\rm bad}(D_i | Y_b, V_b)\, p(q_i=0 | P_b)\Bigr] \times p(m,b,P_b, Y_b,V_b) 
\end{multline}

\begin{multline}
  p(m,b,P_b,Y_b,V_b|\,{\bm D}) \propto \prod_{i=1}^N \Bigl [ [1 - P_b] p_{\rm good}(D_i | m, b)  + P_b p_{\rm bad}(D_i | Y_b, V_b)\Bigr ] \times p(m,b,P_b, Y_b, V_b)\\
  \boxed{
    = \prod_{i=1}^N \left\{ \frac{1 - P_b}{\sqrt{2 \pi \sigma_{yi}^2}} \exp \left[ - \frac{(y_i - m x_i - b)^2}{2 \sigma_{yi}^2} \right] + \frac{P_b}{\sqrt{2 \pi (V_b + \sigma_{yi}^2)}} \exp \left[- \frac{(y_i - Y_b)^2}{2 (V_b + \sigma_{yi}^2)} \right] \right\}
  \times p(m,b,P_b,Y_b,V_b)}
\end{multline}

In it's simplest form (Hogg 2010), the bad data model results in a linear combination of Gaussians. 

A benefit of the bad data model over a chosen likelihood function is that it is motivated. It may still result in the linear combination of Gaussians (nice), but preserves real parameter correlations. You are not forcing the same mean.


\subsection{Spatial correlations of pixels}
We can do more. The bad model of the previous section assumes that each data point is independent. For example, the spectral lines are actually bad, and this means that if a single line is wrong, not just the core of the line (which may be a 10 - 20 sigma residual) will be wrong, but also the wings of the line, which may be only 5 or 3 sigma wrong.

Now, let's come up with a model which includes spatial (wavelength) correlations in the bad data model.

How could we accomplish this? If we identify one point as bad, then the flags around it will have a correlation length, ie, the prior on $q_i$ will be bad. This means that for however many nearby pixels that will be bad (most likely a constant amount for uncrowded lines), then this means that we could still integrate over the qis, it's just that we would now have to take a contribution from the nearest 5 pixels, say. That central pixel would then mark the ``line'' that is bad. 

That is one way to mark the ``badness of the line,'' that the qi are correlated. But we also know that the $y_i$ will be correlated too. 

This correlation length will probably depend on the type of star, and the contribution of the band. For example, if they're simple FGK stars, then the bad lines might be singular. But if they are M stars, then there is a chance that there might be a large ``pseudo-continuum'' region that might be incorrect.

Once we have identified the center of the bad line, then there will be a decrease in the nearby pixels, similar to a Gaussian shape. Presumably we could come up with a correlation in this space too. (or, it could simply be that the correlation between $y_i$ is flat in the case of an M star.

This might actually save the marginalization. We could still do the backwards ``what is the probability a given line is bad.''

For Class I and II errors, we could have a hyper parameter describing the width of each line, and another hyper parameter describing the (distribution of) depth. These could also be sampled in.

For Class III errors, describing how line strengths are off makes sense for stars with continuum, but as we move to later type stars (K stars) that do not have a nice continuum, we may run into trouble. For the later stars (M dwarfs), there might be a whole region that is incorrect.

There are different ways to describe a correlation over pixels. Gaussian random fields. Autocorrelation function.

\subsection{Simple correlation between pixels}
For a simple correlation, say that if a given $q_i$ is bad, then there is also a high probability that the neighboring pixels are also bad. Say we know from inspection that it seems as though bad lines knock out 5 pixel chunks. 


Caveat: I think this assumes that bad lines will not occur within the same spread of each other. Maybe we should include a $P_B$ prior on the good counts too?

\begin{equation}
  p(q_i = 0 | q_{i \pm 5} = 0) = P_B
\end{equation}

\begin{align}
 p(q_i = 0 | q_{i \pm 5} = 0) &= 1/5 \\
 q_{i \pm 4} &= 2/5 \\
 q_{i \pm 3} &= 3/5 \\
 q_{i \pm 2} &= 4/5 \\
\end{align} 

This might need more work with $P_B$.

Then the probability functions would be
\begin{multline}
  p(D_{i + j} | \vt, q_i =0) = \prod_{j = -4, \ne 0}^4 \bigl [ p(D_{i+j} | \vt, q_{i +j}=0) p(q_{i+j} = 0 | q_i=0) + \\
    p(D_{i + j} | \vt, q_{i + j} = 1) p(q_{i +j} = 1 | q_i = 0) \bigr ]
\end{multline}


\begin{multline}
  p(D_{i + j} | \vt, q_i = 1) = \prod_{j = -4, \ne 0}^4 \bigl [ p(D_{i+j} | \vt, q_{i +j}=0) p(q_{i+j} = 0 | q_i=1) + \\
    p(D_{i + j} | \vt, q_{i + j} = 1) p(q_{i +j} = 1 | q_i = 1) \bigr ]
\end{multline}

The marginalization could still be done analytically, but now the likelihood function involves a correlation between adjacent pixels. Could we use a sparse array for this? Or, we could evaluate all of the pixels, and then have the final likelihood function involve some correlation terms between the lnprobs evaluated at each pixel. We could do this using the logarithm so that everything is additive.

Or, what if we evaluated the FFT of the $\log(p(D_i))$ and just the autocorrelation in Fourier space?

\subsection{Correlation between $y_i$}
Not only will the flags be spatially correlated (if a point is bad, the nearby $\sim$ 5 points are also likely to be bad) but for OBAFGK stars, the lines are likely to be Gaussian shape. That is if $q_i$ specifies the center of the line, then the neighbooring $y_{i+1}$ or $y_{i-1}$ will be of lower residual height. This means that the inside of the likelihood function should also have spatial correlations.

Could this correlation be described by a Gaussian random process? \citep{rw05}.

\subsection{Different flags and weights}
Instead of a good or bad flag, what if we had a flag that would identify which class of error the line became? For example we could have the option of $q_i = \{0, 1, 2, 3\}$, where $q_i=0$. 

How do we account for the fact that some parts of the spectrum carry much more information about $\vg$ than other parts of the spectrum? Is it important to weight this parts beforehand? Or should this contribution to the posterior naturally weight itself, the way certain points might heavily contribute to the slope and intercept of a linear fit?

If so, we could imagine having a Gibbs sampler on the weights of each pixel.

Proper Class III modeling is very important, following the discussion of \citep{mga13} Mann et al, and see recent example from Rajpurohit 2103. Some features will not fit perfectly, especially at the lower temperature scale. However, it's important to still take these features int account with the modelling. They also use a $\chi^2$ likelihood function, but perhaps even less appropriate is that they draw error bars by saying that the $\chi^2$ value changes by 5\%. In reality, they are checking the fits by eye and saying whether or not a spectrum matches. This approach is no more justified than a by-eye spectral classification.

\subsection{Hyperparameters}
Difficulty of combining different data sets with different errors \citep{mb13}. Or a hyperparameter like \citep{sdm+07}. Shrinkage similar to \citet{kru10}.

\citet{sdm+07} use ISO SWS infrared spectra. Hierarchical approach. Able to synthesize their own spectra. Use systematic error flags from the instrument, as well as statistical flags. Hierarchical model that uses shrinkage between the observed spectrum and the synthetic spectrum. They do a fit to the variance with a hierarchical linear mixed model (sec 5.2.2, and their figure 7) using design matricies and regression coefficients. This is probably to incorporate some sort of correlation length on the systematic errors, and ``smooth'' the function, as they say. I was having some difficulty understanding it, but it seems as though they were keeping the noise components fixed, and they did not have a contribution to the error budget from regions where the spectral synthesis \emph{itself} contained systematic errors. Perhaps these were masked by the systematic error from the instrument. 

\subsection{Sampling such a model}
If our bad model is sufficiently complex that we are not able to analytically marginalize over the $q_i$, it is still possible to sample in this huge parameter space. A stellar model with flags for each pixel would have $\vstar + N$ parameters, where for a typical optical spectrum $N >$ 100,000. Even using an ensemble sampler, sampling in this space would take a tremendous amount of time to converge. 

An alternative way to approach this problem is to do Gibbs sampling in the flags. It may be possible to do do Gibbs sampling where we draw from the conditional probability distribution, or at the very worst we could still do a Gibbs sampler with a Metropolis-Hastings backend, where we propose a jump in a single $q_i$ at a time. Such an sample requires none of the previous heavy lifting (spectral grid interpolation, FFT, or pixel interpolation), and so even though there are $\sim$ 100,000 flags to sample, this is likely to be computationally feasible.

\section{Injection of fake lines}

An alternative approach is instead of modelling the systematically wrong regions of the spectrum, actually add in lines to the spectrum. This can be thought of as correcting the models. We could add in a set number of lines.

Presumably, each line would be a Gaussian parameterized by $\lambda_0$, strength, and FWHM. Strength could be described by a hyper prior, and FWHM described by a hyper parameter, since all lines should have pretty much the same FWHM (in velocity space). 

In later type spectra, this might actually need to be a larger continuum feature that is added.

We could penalize the model fits by having a hyperprior on the number of bad lines that can be created.

In theory, it is possible to fit the profiles of only certain elements to determine the abundances of certain elements (see citation in conference proceedings of Husser). 

It may be possible to use a linelist from the group that produces the PHOENIX spectra, which describes the element that gave the highest contribution to the opacity in that bin. Now, if the vanadium lines are bad, then we could have a prior that the other vanadium lines might also be bad. We could assemble a piecewise spectrum, where certain pixels are drawn from a different combination of $\vg$, and these pixels had an associated normalization constant so that the piecewise transition was smooth (if, say the bolometric temperature was different). 

Class III errors might be difficult to fit in this approach. However, if the line width does not change dramatically, then we should be able to account for things this way.

\section{Validation tests}
Comparison to main sequence stars of HAT-P-9, and WASP-14, from the TRES archive \citep{tfs+12}.

Comparisons using HIRES data?


\acknowledgments
IC would like to graciously acknowledge Gregory Green, Kaisey Mandel, Doug Finkbeiner, and Daniel Eisenstein for many fruitful discussions about statistics and spectroscopy. This research made use of Astropy, a community-developed core Python package for Astronomy \citep{art+13}.

\appendix

\section{Simplification for small $c_0$ perturbations}
\label{sec:gaussian_simplification}
If the correction in $c_0$ is small, we can use an alternate formulation of $k$ and put Gaussian priors on all nuisance parameters. This allows analytic marginalization over all nuisance parameters (including $c_0$), and reduces the dimensionality of the posterior to only the $\sim 7$ stellar parameters, $\vstar$.

\begin{align}
  k(\lambda | \vN) &= c_0 T_0(\lambda) + c_1 T_1(\lambda) + \ldots + c_N T_N(\lambda) \\
  k(\lambda | \vN) &= \sum^N_{i = 0} c_i T_i(\lambda)
\end{align}
Following \S\ref{sec:nuisance}
\begin{equation}
  \ln \bigl [p(\vt | \vD) \bigr] \propto - \frac{1}{2} \sum_\lambda \left [\frac{\fD(\lambda) - \bigl [ \sum^N_{i = 0} c_i T_i(\lambda) \bigr ]\fM(\lambda | \vstar)  }{\sigma(\lambda)} \right ]^2
\end{equation}
\begin{equation}
 \ln \bigl [p(\vt | \lambda_i) \bigr] \propto -\frac{1}{2 \sigma^2} \bigl [ \fD^2 - 2 \fD \fM \sum_n c_n T_n + \fM^2 \sum_n \sum_m c_n T_n c_m T_m \bigr ]
 \end{equation}
Now the column vectors $\vc$ and $\vec{T}(\lambda)$ include $c_0$ as well
\begin{equation}
  \vc = 
  \begin{bmatrix}
    c_0\\
    c_1\\
    \vdots\\
    c_N
  \end{bmatrix}
  \hspace{3cm}
\vec{T}(\lambda) = 
\begin{bmatrix}
T_0(\lambda)\\
T_1(\lambda)\\
\vdots\\
T_N(\lambda)\\
\end{bmatrix}
\end{equation}
\begin{equation}
  k(\lambda | \vN) = \sum^N_{i = 0} c_i T_i = \vec{T}^\trans \cdot \vc
\end{equation}
\begin{equation}
  {\bm W}(\lambda) = \vec{T} \cdot \vec{T}^\trans = 
  \begin{bmatrix}
T_0 T_0 & T_0 T_1 &  \hdots & T_0 T_N \\
T_1 T_0 & T_1 T_1 &  \hdots & T_1 T_N \\
\vdots  & \vdots  &  \ddots & \vdots \\
T_N T_0 & T_N T_1 &  \hdots & T_N T_N \\
  \end{bmatrix}
\end{equation}
\begin{equation}
  \sum_n \sum_m c_n T_n c_m T_m = \vc^\trans \cdot {\bm W} \cdot \vc
\end{equation}
\begin{equation}
  \ln \bigl [p(\vt | \lambda_i) \bigr] \propto -\frac{1}{2 \sigma^2} \bigl [ \fD^2 - 2 \fD \fM\, \vec{T}^\trans \vc \; + \fM^2 \, \vc^\trans {\bm W}  \vc \bigr ]
\end{equation}
\begin{align}
  {\bm A} &= \sum_\lambda \frac{\fM^2(\lambda)}{\sigma^2(\lambda)} {\bm W}(\lambda)\\
  \vec{B}^\trans &= \sum_\lambda \frac{\fD(\lambda) \fM(\lambda) }{\sigma^2(\lambda)} \vec{T}^\trans(\lambda)\\
  g &= -\frac{1}{2} \sum_\lambda \frac{\fD(\lambda)}{\sigma^2(\lambda)}\\
\end{align}
Where ${\bm A}$, $\vec{B}$, and $g$ are each a function of $\vstar$, $\vN$, and $\vD$. If we include Gaussian priors, then we have 
\begin{align}
  {\bm A}^\prime &= {\bm A} + {\bm D}\\
  \vec{B}^\prime &= \vec{B} + ({\bm D} \vec{\mu})^\trans\\
  g^\prime &= g - \frac{1}{2} \vec{\mu}^\trans {\bm D} \vec{\mu} 
\end{align}
where now ${\bm D}$ and $\vec{\mu}$ include $c_0$ as well. The main difference from before is that now the prior on $c_0$ is now Gaussian instead of log-normal. This is acceptable as long as $\sigma_{c_0}$ is small ($< XX\%$), otherwise this will bias the correction factor. This also means that we will usually have $\vec{\mu} = \{1, 0, \ldots, 0\}$. The full posterior probability function becomes
 \begin{equation}
   \boxed{
  p(\vstar, \vN | \vD) = p(\vstar, \vc\;) \propto \exp \left ( - \frac{1}{2} \vc^\trans {\bm A}^\prime \vc + \vec{B}^{\prime \trans} \vc + g^\prime \right ) p(\vstar) p(\vc)
}
\end{equation}

Using the previous multi-dimensional integral, we can marginalize over all $\vc$ including the Gaussian priors
\begin{equation}
  \boxed{
  p(\vstar | \vD) \propto \sqrt{\frac{(2 \pi )^N}{ {\rm det} \bigl |{\bm A}^\prime \bigr |}} \exp{ \left ( \frac{1}{2} \vec{B}^{\prime\trans} {\bm A}^{\prime -1} \vec{B}^\prime + g^\prime \right )} p(\vstar) 
}
\end{equation}

To gain insight, we can re-express this new version of $k$ in the original form as 
\begin{equation}
   k(\lambda | \vN) = c_0 T_0 + c_1 T_1 + \ldots + c_N T_N
\end{equation}
where $c_1 = c_0 {c_1}^\prime T_0$ and $c_N = c_0 {c_N}^\prime T_0$. Thus, if there needs to be a large perturbation in $c_0$, then there must also be a large perturbation in $c_n$. The prior on $c_1$ is now a product of a log-normal prior on $c_0$ and a Gaussian prior on ${c_1}^\prime$, which is not analytically tractable. Therefore, in the case of large perturbations, it is better to use Equation~ and use a log-normal prior on $c_0$. When we use the original framework, we are saying that perturbations in $c_0$ are small enough that we can approximate the log-normal prior on $c_0$ as Gaussian and that the priors on $c_n$ can also be correctly approximated by a Gaussian. From tests, the original framework gives acceptable accuracy (using $\sigma_{c_n} = 0.05$) if the corrections in $c_0$ are $\sigma_{c_0} \leq 0.35$, or the one-sigma deviation of $c_0$ is between $0.7 \leq c_0 \leq 1.42$. Beyond this, one should switch to this framework, for example to correct for un-flux calibrated data.

%\begin{deluxetable}{ll}
%\tablecaption{\label{table:} Title}
%\tablehead{\colhead{Col1} & \colhead{Col2}}
%\startdata
%\enddata
%\tablecomments{}
%\end{deluxetable}

\bibliography{disks,bayesian,master}
\bibliographystyle{hapj}
\end{document}


